{
  "url": "https://www.law.cornell.edu/regulations/new-york/8-NYCRR-2302.6",
  "title": "N.Y. Comp. Codes R. & Regs. Tit. 8 ยง 2302.6 - State advisory council evaluation",
  "content": "N.Y. Comp. Codes R. & Regs. Tit. 8 ยง 2302.6 - State advisory council evaluation\n\nState Regulations\n(a) In conducting its advisory, evaluation\nand reporting activities, the New York State advisory council is supported by\nthe Office for Program Analysis and Evaluation in the State Education\n\nDepartment. The Office for Program Analysis and Evaluation serves as support\nstaff for the Executive Deputy Commissioner of Education by conducting\nevaluation activities in the areas of policy planning, policy analysis, and\nprogram evaluation. The office's program evaluation function is executed by the\n\nBureau for Program Planning and Evaluation. Major efforts have been made over\nthe past two years by that bureau to develop and implement a model for the\nevaluation of department programs. Results of program evaluations conducted by\nthe bureau are reported directly to the executive deputy commissioner. It is in\nthe context of this program evaluation function that the Bureau for Program\n\nPlanning and Evaluation works with the State advisory council in conducting an\nevaluation of the title IV of the Elementary and Secondary Education Act\nprogram. (b) The organizational\npattern described above places the title IV of the Elementary and Secondary\n\nEducation Act evaluators from the Office for Program Analysis and Evaluation\n(Bureau for Program Planning and Evaluation) in a unique position to assist the\ncouncil in monitoring and evaluating implementation of the title IV of the\n\nElementary and Secondary Education Act program. While individual advisory\ncouncil members are encouraged to conduct assessments of the title IV of the\n\nElementary and Secondary Education Act program on their own, it is recognized\nthat they cannot commit the amount of time and effort necessary to obtain a\ncomprehensive understanding of the title IV of the Elementary and Secondary\n\nEducation Act program. In assisting the council in their evaluation, the title\nIV of the Elementary and Secondary Education Act evaluators will be able to\nobserve the program on a day-to-day, week-to-week basis without having direct\nresponsibility for the administration of the program. This allows the\nevaluators to retain their objectivity while still being knowledgeable of the\nintricacies of the program. In addition, the credibility of the evaluation\nfindings is increased because the evaluators are viewed by both council members\nand program administrators as knowledgeable about program operations. This\nincreased level of credibility should result in greater utilization of\nevaluation data by the advisory council in making decisions on program policy\nand by program administrators in modifying program operations. (c) During the past two Federal fiscal years\n(fiscal year 1976 and fiscal year 1977) the Office for Program Analysis and\n\nEvaluation in the State Education Department has assisted the New York State\nadvisory council in conducting its advisory, evaluation and reporting\nactivities. In both of these years, the State advisory council's evaluation of\ntitle IV of the Elementary and Secondary Education Act has focused on the\nmanagement processes employed in the program. For example, under part B, evaluators have assessed the extent to which LEA part B applications reflect\ncontemporary standards of planning and evaluation for the purpose of improving\n\nState guidelines, forms, and instructions as well as technical assistance given\nto LEA's. In addition, the evaluators worked with part B program administrators\nin developing instruments for the collection of monitoring and evaluation\ninformation on projects. Under part C, evaluators collected and analyzed data\non technical assistance given to local school districts. In addition, evaluators worked with program administrators in devising a system for\nreviewing competitive project proposals submitted under part C. Joint efforts\nbetween the evaluators and program administrators have also resulted in the\ndevelopment of a series of instruments to be used for monitoring the\nimplementation of part C projects. (d) The emphasis on evaluating management\nprocesses under title IV of the Elementary and Secondary Education Act was\nnecessitated by the fact that local district projects were only in the early\nstages of implementation during fiscal year 1976 (i.e. preparing project plans\nand beginning implementation of the plans). During fiscal year 1977 most local\nprojects under parts B and C have been implemented. Procedures for monitoring\nboth B and C projects have been established and implemented. Therefore, evaluation work in fiscal year 1978 will focus on the results of project\nimplementation. Emphasis will be placed on evaluating the outcomes of projects\nand the progress being made in following New York State strategies for part B\nand part C. The next two sections of this evaluation plan will present the\nimplications of this outcome orientation for evaluation of part B and part\nC. (e) Part B--Evaluation strategy. (1)\n\nAs indicated in the fiscal year 1977\nprogram plan, the State advisory council has viewed its part B evaluation\nstrategy as a cumulative strategy; i.e. generalizations on the impact of part B\nprojects in the State can be made only after reviewing data from a number of\nproject years. Since part B is an acquisitions program, the first project year\nin a district is often spent obtaining materials and infusing these materials\ninto the library, guidance program or the curriculum in a subject area. At the\nend of the initial project year, librarians, counselors or teachers may have\nactually used materials with students for only a few months. It is extremely\ndifficult to attribute changes in student behavior to materials which have been\nused for such a short period of time. Therefore, a cumulative strategy in which\ndata is gathered on a project over time will begin to give the council greater\ninsights into the impact of IV-B programs on target populations. Implementation\nof the part B evaluation strategy for fiscal year 1978 will complete the\nfour-phase cumulative approach for the fiscal year 1976 and fiscal year 1977\nprojects. (2)\n\nThe part B evaluation\nstrategy for fiscal year 1978 is designed to complete phases three and four of\nthe strategy outlined in the fiscal year 1977 plan for projects funded with\nfiscal year 1976 funds and fiscal year 1977 funds (see diagram below). For\nprojects supported with fiscal year 1976 funds, phases one and two have been\ncompleted for a sample of districts. The data collection instruments for phases\nthree and four have been developed for use on-site by representatives of the\nIV-B program office. Evaluation work in fiscal year 1978 will concentrate on\nconducting analyses with data gathered on project implementation and impact for\nfiscal year 1976 projects. (3)\n\nIn\nterms of projects funded with fiscal year 1977 monies, phases one and two are\nbeing conducted in spring 1977 (see fiscal year 1977 program plan). Phases\nthree and four will be completed in spring 1978 after projects have been\nimplemented. Prior to collecting data on fiscal year 1977 projects, revisions\nwill be made in the data collection instruments based on experience with the\nfiscal year 1976 projects. Analysis of the data on project implementation and\nimpact for fiscal year 1977 projects will also be conducted in spring\n1978. (4)\n\nThe emphasis on\nevaluating the outcomes of part B projects will be continued for the projects\nfunded with fiscal year 1978 money. The four-phase strategy will be modified\nfor these projects by dropping phase 2--planning quality study--from the\nevaluation design. Work on the financial impact study will be conducted for\nfiscal year 1978 projects in spring 1978. Phases three and four which deal with\nproject monitoring and evaluation will be implemented in spring 1979. The\nspecific objectives and activities of phases three and four for fiscal year\n1978 projects will be included in the fiscal year 1979 program plan since these\nactivities will be conducted during that fiscal year. (f) Phase 1--Financial impact study. The\nobjective of phase 1 is to determine the impact of part B funding under the\ncurrent New York State formula on existing LEA expenditures (from State and\nlocal sources) in areas eligible for funding under the purposes of part B. Information regarding the increase in expenditures (impact) attributed to part\nB will be used by the advisory council in considering possible adjustments to\nthe part B distribution formula. In addition, the impact of alternative local\nschool district expenditure strategies (i.e. how local districts decide to\nspend their part B money such as concentrating the funds on one of the part B\npurposes or on one target group) will be documented. This information will be\nuseful in determining how to maximize the financial impact of part B\nmonies. (g) Phase 2--Planning\nquality study. (1)\n\nThe objective of phase 2\nis to assess the extent to which LEA part B applications reflect contemporary\nstandards of planning and evaluation. The planning quality study focuses on\nwhether LEA's identify their needs, develop programs which are coordinate with\nthose needs and use part B funds to meet those needs. The study is mainly\ndirected at determining the validity of the premise of the part B program that\nLEA's are best aware of their needs in the areas eligible for funding and\ntherefore should be given complete discretion in determining how to spend the\nfunds. The results of the study may result in the advisory council's\nrecommending changes in the part B legislation or regulations. In addition, the\nresults of the study, combined with information on local district expenditure\nstrategies, can be correlated with results of phases 3 and 4 of the evaluation\nplan. Essentially the question is whether local school districts that plan\nprograms in a certain way and expend funds in a certain manner are more likely\nto implement programs which have greater educational impact on\nstudents. (2)\n\nFinally, the results\nof the planning quality study will assist title IV of the Elementary and\n\nSecondary Education Act evaluators and the part B program administrators in\nidentifying local districts with some capability in evaluation. These districts\nwould then serve as prototype districts for developing a reasonable set of\nevaluation requirements for local districts under the part B program. (h) Phase 3--Collection of data on\npart B project implementation. The objective of phase 3 is to determine if a\ndistrict's ability to implement part B projects is related to how money is\nspent and how well programs are planned. Title IV of The Elementary and\n\nSecondary Education Act evaluators and part B program administrators have\njointly determined the criteria by which project implementation will be\nassessed. Data collection instruments will be used to secure necessary\ninformation. The results of these efforts will be reviewed for the sample\ndistricts in terms of results from phases 1 and 2 of the evaluation. (i) Phase 4--Collection of data on impact of\npart B projects in LEA's. The objective of phase 4 is to determine if districts\nwho are more successful in achieving part B objectives employ higher quality\nplanning and use money differently than districts who are less successful in\nachieving part B objectives. Again, the title IV of the Elementary and\n\nSecondary Education Act evaluators and the title IV-B program office have\ndetermined the criteria for measuring project success and the process for\ncollecting empirical data relating quality of planning to project impact. The\nultimate aim of phase 4 is to identify factors associated with successful part\nB projects so program managers at the State and local levels can attempt to put\nthese factors into place. Information from phase 4 activities will also serve\nas a major input into the advisory council's annual evaluation report to USOE\non the overall impact of the title IV-B program. (j) Part C--Evaluation strategy. (1)\n\nSimilar to the evaluation strategy for\npart B, the strategy for part C in fiscal year 1976 and fiscal year 1977\nfocused on the management processes of the program. In those fiscal years, a\nthree-phase evaluation strategy was implemented which was designed to evaluate\nIV-C projects at three points-- initial review and approval of projects, monitoring of project implementation and evaluation of effectiveness of\nprojects. As a result of cooperative efforts between the evaluators and the\nIV-C program office criteria and procedures for project review and approval\nwere established, procedures and data collection instruments for project\nmonitoring were developed and designs for evaluating project effectiveness were\ndevised. The evaluation strategy for fiscal year 1978 will emphasize evaluating\nthe progress being made by IV-C projects in each of the State priority areas in\nfulfilling the transferring success strategy of the program. (2)\n\nThe fiscal year 1978 part C evaluation\nstrategy will assess projects in each of the three grant types--developer, validation and demonstration both in terms of the effectiveness of the\nindividual project and in relation to the progress that project has made in\nfulfilling the IV-C program strategy. (i) Developer grants. (a) Developer grants are\nawarded to LEA's for the development of new programs aimed at meeting needs\ncommon to many school districts in the State. Developer grants are multi-year; funded on a year-by-year basis with a project generally operating for three\nyears as a developer. Three different levels of evaluation will be addressed\nfor developer grants under the fiscal year 1978 evaluation strategy. (b) The first level of evaluation is judging\nthe effectiveness of the IV-C project for the LEA. Effectiveness will be judged\nbased on the degree of implementation of project activities and achievement of\nproject objectives as planned in the project proposal. The source of\ninformation on implementation of project activities will be progress reports\nand follow-up site visits. The source of information on achievement of project\nobjectives will be the annual evaluation report which is based on the proposed\nevaluation design. Project effectiveness will be assessed at the end of each\nproject year for developer grants. The project effectiveness evaluation will be\nused in conjunction with the proposal for project continuation to determine\nwhether or not a developer project will be funded for an additional\nyear. (c) The second level of\nevaluation for developer grants is assessing the impact of the project on the\ntarget population. Impact of the project will be judged in terms of changes in\nthe behavior of target populations (e.g. gains in student achievement, changes\nin teacher performance, dollar savings attributed to the project). The source\nof this information on project impact is the annual evaluation report of the\nproject. However, information on project impact will probably not be available\nat the end of the initial year of the project. Project impact evaluation will\noccur at the end of the second year of the project (these data will be used in\nmaking decisions on continuing to fund the project) and at the end of the third\nand final year of the project (these data will be used as an input to making\ndecisions on validating the project). (d) The third level of evaluation of\ndeveloper grants is assessing the progress of that project in moving to the\nsucceeding phases of the IV-C transferring success strategy. For each priority\narea, the percentage of projects being validated can be calculated. In\naddition, the cost-effectiveness of developer grants in each priority area can\nbe determined by calculating the total investment of Federal (and possibly\nlocal) funds and comparing that investment to the number of projects receiving\nvalidation and eventually becoming demonstrators (e.g. cost/validation and\ncost/demonstration). (ii)\n\nValidation grants. (a) Validation grants are given to LEA's to\ncover costs necessary for gathering data on an already existing program that is\nof high quality, but where there is not enough hard data for validation. Validation grants are designed to cover costs for only one year, after which a\ndistrict is expected to submit its project for validation. (b) The evaluation strategy for validation\ngrants combines the project effectiveness level discussed under developer\ngrants with assessing the progress of the project in moving to the next phase\nof the IV-C transferring success strategy. In terms of validation grants, the\nproof of project effectiveness is the project's ability to be validated. A\nvalidation grant was originally given to the LEA because it provided evidence\nthat it did not have sufficient information to enable the project to be\nvalidated. Therefore, effective use of the funds should result in a project\nbeing validated. As was the case for developer grants, the cost-effectiveness\nof validation grants can be measured for a priority area based on the total\ninvestment of Federal (and probably local) funds per project being validated\n(cost/validation) and eventually becoming a demonstrator\n(cost/demonstration). (iii)\n\nDemonstration grants. (a) Demonstration grants are given to\ndistricts with State and/or nationally validated programs to enable them to\ninform, assist and train potential adopters. Demonstration funds are granted on\na year-to-year basis with a typical project operating for two years. Three\ndifferent levels of evaluation will be addressed for demonstration grants under\nthe fiscal year 1978 evaluation strategy. (b) The first level of evaluation is a\nprocess evaluation of demonstration activities leading to approval of contracts\nwith replicators. These demonstration activities include sending out first and\nsecond level awareness materials, contacting replicators through workshops and\non-site visitations, selection of replicators and the formulation of a contract\nbetween the demonstrator and replicator for services which is subject to the\napproval of the State Education Department. The source of information for the\nprocess evaluation will be the demonstrator's progress report which documents\nimplementation of the above activities. In addition, the costs (Federal and\nlocal) of the demonstration activities will be calculated enabling evaluators\nto determine the cost per replication contract. (c) The second level of evaluation for\ndemonstration grants is also a process evaluation, but focuses on demonstration\nactivities leading to installation of the exemplary project in the replicating\ndistrict. These demonstration activities include training staff from the\nreplicator district, providing assistance and advice to staff from the\nreplicating district and providing any other follow-up information or\nconsultation requested by the replicating district once the project has been\ninstalled in the district. The sources of information for the second level of\nevaluation will be the demonstrator's progress report, the annual evaluation\nreport of demonstrators and possibly a progress report from\nreplicators. (d) The third level of\nevaluation for demonstration grants is based on the actual adoption/adaption of\nIV-C projects. Adoption/adaption will be measured in three ways. The first\nmeasure of adoption/adaption deals with quantity. For this measure the number\nof districts implementing replications will be calculated. In addition, the\nnumber of pupils served by districts replicating exemplary programs will be\ndetermined. The quantity measures will be aggregated by priority area. The\nsources of information for the quantity measure will be the demonstrator's\nprogress reports, the annual evaluation report of demonstrators as well as\nsimilar documents from replicators. (e) Another measure of adoption/adaption\ndeals with quality of the adoption/adaption. Quality will be assessed in terms\nof whether the key elements of the exemplary project are in place in the\nreplicating district. The demonstrator will be asked to specify the key\nelements of the exemplary project which should be present in replicating\ndistricts. The source of information to assess quality of the adoption/adaption\nwill be on-site visits to a sample of replicator districts. (f) The third measure of adoption/adaption\nwill address the results of adoption/adaption. Results will be addressed in\nterms of whether the replicating district has in fact achieved results with\ntarget populations (e.g. student achievement, changes in behavior of teachers, dollar savings) comparable to the demonstrating district. The source of\ninformation for the results measure will be the evaluation report from the\nreplicator. (g) All three of the\nmeasures comprising the third level of evaluation for demonstration grants can\nbe compared to cost figures as a means of conducting cost-effectiveness\nanalysis. Therefore, cost-effectiveness for demonstration grants will be based\non cost/quantity or number of replications, cost/quality or number of\nreplications containing certain key elements of the original project and\ncost/results achieved by demonstrators. (k) Part C--Strengthening\nevaluation. (1)\n\nAt the December 1976 meeting\nof the State advisory council, the committee on evaluation reviewed the\nevaluation reports from the title IV-C strengthening projects for inclusion in\nthe title IV of the Elementary and Secondary Education Act annual report for\nfiscal year 1976. The committee expressed a need to improve the process for\nassessing the effectiveness of strengthening projects. Projects operating in\nfiscal year 1976 were required to complete a self-evaluation report presenting\nthe achievements and accomplishments of the project in that year. This strategy\nwas also planned for fiscal year 1977. While members of the committee found the\nself-evaluation reports to be informative, many members expressed a need to\nreview some of the more than 25 strengthening projects in depth. The evaluation\nstrategy for strengthening projects in fiscal year 1978 will include both the\nself-evaluation mechanism completed by project managers and in-depth\nevaluations of a limited number of projects conducted by staff from the Office\nof Program Analysis and Evaluation. (2)\n\nDuring fiscal year 1978, the State\nadvisory council will establish a set of priority projects for in-depth\nevaluation. These priorities will be reviewed by the Executive Deputy\n\nCommissioner of Education who will make the final decision on which projects\nwill be evaluated during the fiscal year. The number of evaluations will depend\non available staff resources. The in-depth evaluations will be conducted using\nan evaluation model developed by the Office of Program Analysis and Evaluation\nfor reviewing Education Department programs. The evaluation model was developed\nduring 1974 and 1975. The first version of the model was field-tested in early\n1975 and revised in May 1975. The model has subsequently been used in 1975 and\n1976 to review programs in the State Education Department. (3)\n\nThe evaluation model is designed to\nevaluate both the managerial quality and the effectiveness of programs in the\n\nEducation Department. Since most department programs do not directly serve\nclients (i.e. students) the examination of client outcomes would only have\nlimited utility with respect to the operation of programs that do not directly\nserve the clients. Consequently, the model places heavy emphasis on reviewing a\nprogram from the point of whether or not the elements of good management can be\nfound. These elements have been specified by the department and consist of such\nthings as: (i) a well-defined program\ndirection including specification of client needs, objectives for clients, conditions required for clients to realize the objectives and activities that\nthe program will undertake to establish the conditions; (ii)\nconsensus of staff concerning program\ndirection; (iii)\nevaluation which\nassesses the extent of activity implementation, condition establishment and the\nrealization of objectives for clients. Notes\nN.Y. Comp. Codes\nR. & Regs. Tit. 8\nยง\n2302.6\n\nState regulations are updated quarterly; we currently have two versions available. Below is a\ncomparison between our most recent version and the prior quarterly release. More comparison features will be added as we have more versions to compare. No prior version found.",
  "url_type": "regulation",
  "source_index": 855
}