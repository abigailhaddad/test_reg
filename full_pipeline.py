#!/usr/bin/env python3
"""
Complete NY Regulations Pipeline - From 0 to Individual Clean Files
Runs the entire process: scrape -> clean -> extract individual files
"""

import sys
import subprocess
import signal
import os
import json
import time
import re
from pathlib import Path
from ny_regulations_scraper import NYRegulationsScraper

class FullPipeline:
    def __init__(self):
        self.caffeinate_proc = None
        
    def start_caffeinate(self):
        """Start caffeinate to prevent system sleep"""
        try:
            print("ğŸ”‹ Starting caffeinate to prevent system sleep...")
            self.caffeinate_proc = subprocess.Popen(['caffeinate', '-d'], 
                                                  stdout=subprocess.DEVNULL, 
                                                  stderr=subprocess.DEVNULL)
            print("âœ… Caffeinate started successfully")
        except Exception as e:
            print(f"âš ï¸  Could not start caffeinate: {e}")
    
    def stop_caffeinate(self):
        """Stop caffeinate process"""
        if self.caffeinate_proc:
            try:
                self.caffeinate_proc.terminate()
                self.caffeinate_proc.wait(timeout=5)
                print("ğŸ”‹ Caffeinate stopped")
            except:
                try:
                    self.caffeinate_proc.kill()
                except:
                    pass
    
    def setup_signal_handlers(self):
        """Setup clean shutdown handlers"""
        def signal_handler(sig, frame):
            print("\nğŸ›‘ Received interrupt signal, cleaning up...")
            self.stop_caffeinate()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def step1_scrape_regulations(self, max_pages=None):
        """Step 1: Scrape all regulations"""
        print("=" * 60)
        print("ğŸ“¥ STEP 1: SCRAPING NY REGULATIONS")
        print("=" * 60)
        
        scraper = NYRegulationsScraper()
        
        print("ğŸš€ Starting comprehensive scrape...")
        print("Features: caffeinate, auto-resume, retry logic, progress saving, content cleaning")
        print()
        
        if scraper.visited_urls:
            print(f"ğŸ“ˆ Resuming from {len(scraper.visited_urls)} previously visited URLs")
        
        start_time = time.time()
        data = scraper.scrape_all(max_pages=max_pages)
        end_time = time.time()
        
        if not data:
            print("âŒ No data was scraped")
            return False
            
        # Save raw scraped data
        scraper.save_to_json(data, 'ny_regulations_complete.json')
        
        elapsed = end_time - start_time
        print(f"\nâœ… Step 1 Complete: {len(data)} regulations scraped")
        print(f"â±ï¸  Time: {int(elapsed//3600)}h {int((elapsed%3600)//60)}m")
        
        return data
    
    def step2_process_cache_to_clean_data(self):
        """Step 2: Convert cached HTML to clean regulation data"""
        print("\n" + "=" * 60)
        print("ğŸ§¹ STEP 2: PROCESSING CACHED FILES TO CLEAN DATA")
        print("=" * 60)
        
        scraper = NYRegulationsScraper()
        cache_dir = Path('scraped_data')
        
        if not cache_dir.exists():
            print("âŒ No scraped_data directory found")
            return False
        
        # Find all cached HTML files
        html_files = list(cache_dir.glob('*.json'))
        regulation_data = []
        
        print(f"ğŸ” Processing {len(html_files)} cached files...")
        
        processed_count = 0
        for i, html_file in enumerate(html_files):
            if html_file.name in ['progress.json', 'failed_urls.json']:
                continue
                
            try:
                with open(html_file, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                
                if 'html' in cached_data and 'url' in cached_data:
                    url = cached_data['url']
                    html = cached_data['html']
                    
                    # Convert to clean regulation data
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Extract title
                    title_elem = soup.find('h1') or soup.find('title')
                    title = title_elem.get_text(strip=True) if title_elem else "No title found"
                    
                    # Extract actual regulation content (not navigation/ads)
                    content = ""
                    
                    # Look for the actual regulation text container
                    reg_content = soup.find('div', class_='statereg-text')
                    if reg_content:
                        content = reg_content.get_text(separator='\n', strip=True)
                    else:
                        # Fallback to main content
                        main_content = soup.find('div', {'id': 'content'}) or soup.find('main')
                        if main_content:
                            for unwanted in main_content(['nav', 'aside', 'footer', 'script', 'style', 'noscript', 'header']):
                                unwanted.decompose()
                            content = main_content.get_text(separator='\n', strip=True)
                    
                    # Clean the content
                    cleaned_content = scraper.clean_regulation_text(content)
                    
                    regulation = {
                        'url': url,
                        'title': title,
                        'content': content,
                        'cleaned_content': cleaned_content,
                        'url_type': scraper.classify_url(url)
                    }
                    
                    regulation_data.append(regulation)
                    processed_count += 1
                    
                    if processed_count % 500 == 0:
                        print(f"  Processed {processed_count} files...")
                        
            except Exception as e:
                continue
        
        if not regulation_data:
            print("âŒ No valid regulation data found")
            return False
        
        # Save clean regulation data
        with open('ny_regulations_from_cache.json', 'w', encoding='utf-8') as f:
            json.dump(regulation_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Step 2 Complete: {len(regulation_data)} regulations with cleaned content")
        print(f"ğŸ’¾ Saved to ny_regulations_from_cache.json")
        
        return regulation_data
    
    def step3_create_individual_files(self):
        """Step 3: Create individual JSON files from cleaned data"""
        print("\n" + "=" * 60)
        print("ğŸ“ STEP 3: CREATING INDIVIDUAL REGULATION FILES")
        print("=" * 60)
        
        # Load the clean data
        try:
            with open('ny_regulations_from_cache.json', 'r', encoding='utf-8') as f:
                regulations = json.load(f)
        except FileNotFoundError:
            print("âŒ ny_regulations_from_cache.json not found. Run steps 1-2 first.")
            return False
        
        print(f"ğŸ“„ Loaded {len(regulations)} regulations")
        
        # Create output directory
        output_dir = Path('individual_regulations')
        if output_dir.exists():
            print("ğŸ—‘ï¸  Removing old individual files...")
            import shutil
            shutil.rmtree(output_dir)
        output_dir.mkdir()
        
        print("ğŸ“ Creating individual files...")
        
        created_count = 0
        for regulation in regulations:
            url = regulation['url']
            
            # Create clean filename
            url_part = url.split('/')[-1] if '/' in url else url
            clean_name = re.sub(r'[^\w\-\.]', '_', url_part)
            filename = f"{clean_name}.json"
            
            # Create individual file with cleaned content
            individual_file = {
                'url': regulation['url'],
                'title': regulation['title'],
                'content': regulation['cleaned_content'],  # Use cleaned version!
                'url_type': regulation.get('url_type', 'unknown')
            }
            
            file_path = output_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(individual_file, f, indent=2, ensure_ascii=False)
            
            created_count += 1
            if created_count % 1000 == 0:
                print(f"  Created {created_count}/{len(regulations)} files...")
        
        print(f"âœ… Step 3 Complete: {created_count} individual files created")
        print(f"ğŸ“ Location: {output_dir}/")
        
        return True
    
    def run_full_pipeline(self, max_pages=None):
        """Run the complete pipeline"""
        print("ğŸ—½" * 20)
        print("NY REGULATIONS COMPLETE PIPELINE")
        print("From 0 to Individual Clean Files")
        print("ğŸ—½" * 20)
        print()
        
        self.setup_signal_handlers()
        self.start_caffeinate()
        
        try:
            # Step 1: Scrape
            data = self.step1_scrape_regulations(max_pages)
            if not data:
                return False
            
            # Step 2: Clean
            clean_data = self.step2_process_cache_to_clean_data()
            if not clean_data:
                return False
            
            # Step 3: Individual files
            success = self.step3_create_individual_files()
            if not success:
                return False
            
            print("\n" + "ğŸ‰" * 20)
            print("PIPELINE COMPLETE!")
            print("ğŸ‰" * 20)
            print(f"âœ… {len(clean_data)} regulations scraped and cleaned")
            print(f"ğŸ“ Individual files: individual_regulations/")
            print(f"ğŸ“„ Master file: ny_regulations_from_cache.json")
            
            return True
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ Pipeline interrupted by user")
            return False
        except Exception as e:
            print(f"âŒ Pipeline failed: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            self.stop_caffeinate()

def main():
    pipeline = FullPipeline()
    
    # Parse arguments
    max_pages = None
    auto_confirm = False
    
    if '--test' in sys.argv:
        max_pages = 100
        auto_confirm = True
        print("ğŸ§ª Test mode: limiting to 100 pages")
    elif '--small' in sys.argv:
        max_pages = 1000
        print("ğŸ“Š Small run: limiting to 1000 pages")
    
    if '--yes' in sys.argv:
        auto_confirm = True
    
    # Confirm before starting
    if not auto_confirm:
        response = input("Run the complete pipeline from scratch? This will take several hours. (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("Pipeline cancelled.")
            return
    
    # Run it
    success = pipeline.run_full_pipeline(max_pages)
    
    if success:
        print("\nğŸ¯ Ready to use! Check individual_regulations/ directory")
    else:
        print("\nâŒ Pipeline failed")

if __name__ == "__main__":
    main()